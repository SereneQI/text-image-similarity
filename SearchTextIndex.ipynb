{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "from PIL import Image as PILImage\n",
    "import lmdb\n",
    "import torch \n",
    "import io\n",
    "from misc.model import joint_embedding\n",
    "from torchvision import transforms\n",
    "from misc.utils import save_obj, collate_fn_cap_padded, encode_sentence\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "index_path = \"flickr30k_flat_full.index\"\n",
    "h5_path = \"embeddings_sentences_full.h5\"\n",
    "model_path = \"weights/best_correct_full_cs.pth.tar\"\n",
    "dict_path = \"/data/m.portaz/wiki.multi.en.vec\"\n",
    "\n",
    "index = faiss.read_index(str(index_path))\n",
    "index_search = index.search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "join_emb = joint_embedding(checkpoint['args_dict'])\n",
    "join_emb.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "for param in join_emb.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "join_emb.to(device)\n",
    "join_emb.eval()\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "prepro_val = transforms.Compose([\n",
    "    transforms.Resize((400, 400)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "\n",
    "tf_img = prepro_val\n",
    "\n",
    "def load_vec(emb_path):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "embeddings, id2word, word2id = load_vec(dict_path)\n",
    "\n",
    "def get_features_sentence(sentence):\n",
    "    input_sentence = encode_sentence(sentence, embeddings, word2id)\n",
    "    input_caps = input_sentence.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, output_emb = join_emb(None, input_caps, [len(sentence.split(' '))])\n",
    "    return output_emb.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentences(uuids):\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        sentences = f['sentence']\n",
    "        for caption_id in uuids[0]:\n",
    "            print(sentences[int(caption_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printNeighboursSentence(sentence,  nb_neigbours):\n",
    "    embedding_words = get_features_sentence(sentence)\n",
    "    dists, uuids = index_search(embedding_words, nb_neigbours)\n",
    "    getSentences(uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ein lächelnder junger mann geht in der nähe des strandes vorbei und trägt dabei eine baseballkappe , ein blaues t-shirt und jeans .\n",
      "\n",
      "a barefoot boy with a blue and white striped towel is standing on the beach .\n",
      "\n",
      "a man on a beach building a sand castle .\n",
      "\n",
      "a smiling young man walking on next to the beach wearing a baseball cap , blue t-shirt and jeans .\n",
      "\n",
      "ein mann an einem strand baut eine sandburg .\n",
      "\n",
      "un jeune homme souriant , marchant au bord de la plage avec une casquette , un t-shirt bleu et un jean .\n",
      "\n",
      "ein barfüßiger junge mit einem blau-weiß gestreiften handtuch steht am strand .\n",
      "\n",
      "ein mann rasiert sich , während er am strand vor dem meer sitzt .\n",
      "\n",
      "eine person mit neonpinkfarbenem haar und gelbem t-shirt blickt über einen strand .\n",
      "\n",
      "un homme se rase , assis sur la plage devant l&apos; océan .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printNeighboursSentence('a man walks on a beach', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_image(image_path):\n",
    "    img = PILImage.open(image_path, mode=\"r\")\n",
    "\n",
    "    if img.mode is 'L':\n",
    "        rgbimg = Image.new(\"RGB\", img.size)\n",
    "        rgbimg.paste(img)\n",
    "        img = rgbimg\n",
    "    if img.mode is not 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "\n",
    "    img = tf_img(img)\n",
    "\n",
    "    input_img = img.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output, _ = join_emb(input_img, None, None)\n",
    "    return output.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printNeighboursImages(image_path, nb_neigbours):\n",
    "    embedding_image = get_features_image(image_path)\n",
    "    edists, uuids = index_search(embedding_image, nb_neigbours)\n",
    "    getSentences(uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man on a beach building a sand castle .\n",
      "\n",
      "a smiling young man walking on next to the beach wearing a baseball cap , blue t-shirt and jeans .\n",
      "\n",
      "a man sits alone fishing along the shoreline .\n",
      "\n",
      "a barefoot boy with a blue and white striped towel is standing on the beach .\n",
      "\n",
      "a man is shaving while sitting on the beach in front of the ocean .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printNeighboursImages('/data/a.nivaggioli/features/manWalkingOnBeach.png', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
